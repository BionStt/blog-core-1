[
  
    {
      "title"    : "Stepping up your dev game !",
      "category" : "general",
      "content"  : "In every developer time, there comes a time when he has to set-up his new working environment. For me, the transition from Windows powered PC to a Mac OSX powered laptop was one of the best things I did in my career. I found the openness of things you can do on a Unix based system accompanied with a solid appealing set of software.However, setting everything the way you want it can be time consuming, so i decided after few trials and errors to share my experience in customizing my machine and how it can be easily transferred into other machines as well.Customizing the UI MacOSX Custom layout and widgetsFirst, lets start with the light weight things. The desktop and general UI enhancement. I always love to show some nice widgets on my desktop, for that i am using Übersicht. I have used Geektool which basically allows you to run custom scripts and display them on the desktop, but i wasn’t at all happy with the experience of styling those “widgets” and the availability of 3rd party widgets.Übersicht lets you run system commands and display their output on your desktop in little containers, called widgets. Widgets are written using HTML5, which means they are:  Easy to write and customize  Can show data in tables, charts, graphs … you name it  Can react to different screen sizesThere is already a nice set of widgets published in the gallery. In my widgets folder, you will find the set of widgets i am using with customized positioning and styling.Widgets  simple-date for the date and time in the middle of the screen  prayer to show the Muslim prayer times for my area, the green indicating the current active prayer time  weather to show the nice weather forecast. It uses the Free Geo IP API to obtain your location, and the Yahoo Weather API to obtain the weather information.  System Stats disk-usage: Shows disk usage for mounted disks, with the option to exclude specific disks.network-info: Displays current Ethernet and Wi-Fi status (connected / not connected / IP Address / Mac Address.pubic_ip: Displays the current public IP address.swap-usage: Display swap file metrics for your system.top-cpu: Shows CPU usage summary.top-mem: Shows a memory usage summary.network-throughput: Shows incoming and outgoing throughput on a user-specified network interface.total-mem: Total system memory benchmarks.Installation  Installing Übersicht can be bone using the downloadable zip file or the recommended using would be via cask (which i will explain later) via brew cask install ubersicht  Installing the widgets is done by copying the contents of the widgets folder into Library -&amp;gt; Application Support -&amp;gt; Übersicht -&amp;gt; widgets. However, what i do is i setup my widgets to be in my Github repo or dropbox folder so that they are synced and can be backed up and i link the folder directly to the desired location via ln -s ~/Projects/Configurations/config/widgets/ ~/Library/Application Support/U¨bersicht/widgets where the first parameter is the location of the widgets/ folder cloned.Note I have included the folder link in my .osx file, you may need to adjust the path of the first argument.Other WidgetsOne element that i use that is not part of Übersicht is Dateline. It is a discrete desktop calendar with support to iCal and other calendar applications.GeekletsIn the geeklets/ folder there is a backup of the deprecated Geeklet scripts that i used with the old Geektool. If you still prefer it over Übersicht then feel free to use them.Dock &amp;amp; Desktop IconsIn the flat-icons you will find the set of flat icons i have used for my mounted HDD icons and the dock.To easily change application icons, i suggest you use LiteIcon which can be also installed via my caskfile.shTo further customize the dock, i use Dockmod which is the most feature-rich application for dock theming with support up to OSX Yosemite in order to provide a transparent Dock background. Dockmod can be also installed via my caskfile.I also remove the arrow icons for aliases/shortcuts which can be done automatically when executing my .osx file or manually by typing the following command in the terminal:mv /System/Library/CoreServices/CoreTypes.bundle/Contents/Resources/AliasBadgeIcon.icns /System/Library/CoreServices/CoreTypes.bundle/Contents/Resources/AliasBadgeIcon_OFF.icnsFinderAlthough i have the latest OSX Yosemite update, i am still unhappy with the capabilities of the Finder. I have found that PathFinder is a great replacement. Just head over and you can check the list of things you can do with this great piece of software.Other Tools  Flexiglass: Flexiglass features complete and convenient window management with exclusive multi-touch gestures support, offering a simple way to manage many windows on a Mac with a mouse, keyboard, trackpad, and graphics tablet. It includes different tools to move, resize, and arrange windows on the screen easily and joyfully.  Bartender: Bartender lets you organize your menu bar apps, by hiding them, rearranging them, or moving them to Bartender’s Bar. You can display the full menu bar, set options to have menu bar items show in the menu bar when they have updated, or have them always visible in Bartender’s Bar.  Alfred: Alfred saves you time when you search for files online or on your Mac. Be more productive with hotkeys, keywords and file actions at your fingertips. It is super useful with the powerpack activated with features like clipboard history. Check also this set of workflows for more integrations.  Caffeine: Caffeine is a tiny program that puts an icon in the right side of your menu bar. Click it to prevent your Mac from automatically going to sleep, dimming the screen or starting screen savers.  Flashlight: Flashlight is an unofficial Spotlight API that allows you to pro grammatically process queries and add additional results. It’s very rough right now, and a horrendous hack, but a fun proof of concept.  uBar: uBar aims at replacing the Mac Dock. It helps you find the window you’re looking for at a glance. If an application has more than one window, you’ll know right away. Click that application to open the window menu, which allows you to bring any window or even just the application itself to focus. You can turn window grouping off, and all your individual windows will be shown right in uBar.Alfred WorkflowsWorkflows are one of the key features in Alfred v2; They provide a way for you to extend Alfred to perform the tasks you need more efficiently.For example, with a workflow, you can launch a group of applications from a hotkey, create custom search filters for frequent searches or run scripts from a keyword.The Workflows i use are (Located in the workflows folder):  Fantastical: Allow for easy adding of calendar events.  Kill Process: When a process hangs, it’s a hassle to open Activity Monitor, search for it, and kill it. Once again, Alfred makes a repetitive task much faster.  Shorten URL: Shortening URLs (for tweets, emails, etc.) is a pain; you have to go to the website you want, enter your URL, wait for the transformation to occur, copy it, and move on. This allows for a quicker process across all the famous URL shorteners.  StackOverflow: Search StackOverflow questions directly.  Encode/Decode:: Encoding and decoding a string into multiple variations  Github: A bunch of Github commands integrations for Alfred.  Search Safari and Chrome Tabs: Search tabs in Safari and Chrome (also supports WebKit, Chromium, and Chrome Canary)Workflows ReferenceSetting-up the MachineDevelopers are very picky about their working environment. We may consolidate various . files tailed for our tastes over years and track the change in a version control system. It’s no secret that on the UNIX world, dotfiles play a very important part when it comes to making your terminal look good. Be it on Linux, be it on a Mac. Dotfiles are there so you can configure your favorite software to look just the way you like it.Dotfiles can be used to customize the look of the terminal, to manage bundles with Vim and saving configuration for almost anything.One thing that normally annoys me, is the fact that whenever I rebuild my machine (or build a new one) I need to copy over my dotfiles, and obviously make sure they are kept up-to-date on all my devices when I change something.My configuration consists of the following:  Mac OSX terminal with bash shell configured with a forked bash-it configurations  iTerm2 terminal with zsh shell configured with a forked oh-my-zsh configurations  A cloned gitignore repo for easy fetching of .gitignore files into various projects  A Custom set of dotfiles for various machine-wide configurationsInstallationFirst of all you will have to clone this repository into your local machine, and since there are a bunch of other git modules in this repo, you will need to recursively clone this repository and all of it children using:git clone --recursive http://github.com/ahmadassaf/ConfigurationsNote If you have cloned without –recursive, you may find out that some folders are empty. To fix this:git submodule update --initAfterwards you can launch the installation by running sh install.sh. What this will do is:  It will ask you if you would like to run an update using the update.sh script. Usually this is good when you have lots of changes in your repos and you wish to fetch all the changes afterwards, but simply skip this if this is a fresh installation.  It will install the oh-my-zsh scripts by symlinking the folder and the zshrc into the ~ folder.  It will prompt if you wish to install zsh-syntax-highlighting and does that automatically.  It will install the bash-it script files and guide throughout the installation process.  It will prompt if you would like to add the Aliases defined also to your zsh aliases recommended  It will symlink the dotfiles into your ~ folder.  It will prompt if you wish to install type-based ls  It will prompt if you wish to install grc for command output coloringIt will install homebrew and the following brew formulas: Core Modules coreutils, moreutils, findutils, binutilsBash, Bash plugins bash, bash-completion, zsh, grcDevelopment git, heroku, node, mercurial, iojsGit speific addons git-extras, hub, bfgUtilities htop-osx, id3tool, lesspipe, ssh-copy-id, the_silver_searcher, tree, fcrackzip, foremost, ack, p7zip, pigz, pv, gnu-sed, vim, wgetNetwork tools dns2tcp, knock, rename, webkit2pngDupes grep, screenIt will install cask and the following applications: Utilities path-finder, alfred, cakebrew, a-better-finder-rename, fastscripts, bettertouchtool, iterm2, launchrocket, gyazo, ubersicht, dockmod, liteicon, cleanmymac, bartender, flexiglassProductivity todoist, evernote, mendeley-desktop, fantastical, slack, screenheroDevelopment sublime-text3, atom, sourcetree, github, brackets, coda, tower, transmission-remote-gui, macvim, sqlite-database-browser, lightpaper, codebox, anvil, hex-fiend, ksdiff, codekit, MAMPInternet, Networking firefox, google-chrome, charles, dropbox, google-drive, mailbox, viber, utorrent, plex-media-serverMisc vlc, imagealpha, imageoptim, steam, the-unarchiver, spotify, handbrakeQuick Look plugins betterzipql, qlcolorcode, qlmarkdown, qlprettypatch, qlstephen, quicklook-csv, quicklook-json, quicknfo, suspicious-package, webp-quicklookColor pickers colorpicker-developer, colorpicker-skalacolo  It will aggregate the iTerm and Terminal themes as prompt you for the name of the color scheme you want to apply and apply it for you  It will prompt if you would like to run the .osx file which contains some system wide modifications and runs it for you.I recommend that you read the .osx file and enable or disable the features that you like. I also create some symlinks there for my Sublime Text 3 configurations and the widgets of Übersicht.  It will install the powerline fonts  It will prompt if you wish to set the bash to the updated version installed by HomebrewUpdatesSimply run sh update.sh and it will take care of the rest",
      "url"      : "/setting-up-a-superhero-development-environment/",
      "date"     : "2015-05-23 00:00:00 +0100"
    },
  
    {
      "title"    : "dotfiles for the public",
      "category" : "general",
      "content"  : "What are Dotfiles?If you’re not familiar with the concept of dotfiles, check out Github’s dotfiles page to learn more about them. Essentially, when someone says “dotfiles” they mean maintaining your command-line preferences in a Git repository (sort of like how I use Dropbox to manage my preference files for TextExpander, etc.) that you install on every computer.The name dotfiles refers to the fact that most of the files that perform this sort of configuration start with a dot. The Zsh configuration file, for example, is .zshrc The SSH configuration folder is .ssh And so on. So the concept of “dotfiles” just means “versioning your configuration files.”Your dotfiles will help you create powerful and consistent shell shortcuts and functions, settings for your editors, color coding and layouts for your shell, preferences and authentication for ssh and mysql and other protocols, and more.Warning: If you want to give these dotfiles a try, you should first fork this repository, review the code, and remove things you don’t want or need. Don’t blindly use my settings unless you know what that entails. Use at your own risk!Superhero Dotfiles and Their Super PowersDotfiles are split into two main types. Those that contain a set of commands and only run once, .osx for example runs a list of commands and gives OS X super powers. Other files such as .bash_profile and .bashrc run each time you open a new Terminal session and gives your Terminal super powers.Here’s a run down of the dotfiles in my repo and a description of what they can do..bash_profile / .bashrcWhen you open a new Terminal session, this file is loaded by Bash. It loads in the other dotfiles path,bash_prompt,exports,aliases,functions,extra and configures some useful settings such as auto correcting typos when using cd completion.In some instances .bashrc can be loaded, so this file makes sure that .bash_profile is called..pathThis file speeds up the process of running executable files. Rather than having to cd back and forth across various paths to executable files, you can set the file paths in your .path dotilfe and then run executable files directly.Generally, this file isn’t held in the public repo as it can contain sensitive information.Here’s an example ~/.path file that adds ~/utils to the $PATH:export PATH=&quot;$HOME/utils:$PATH&quot;`.bash_promptUsing this file you can customise and set the various colors of your Bash prompt..exportsSets environment variables, such as setting Vim as the default editor using export EDITOR=&quot;sublime&quot; It also increases the amount of history saved, useful for backtracking over previous commands you’ve used..aliasesThis file contains useful aliases to help you write less. For example, instead of typing cd .. you can set it here to be ‘..’. Starting to like these files yet?.functionsSimilar to aliases, except functions can take arguments.Before when I mentioned I was looking over different dotfile repos, I did mkdir to create a directory. After that, I’d then need to cd into that directory..gitconfigThis file is only used by Git, for example, when a git command is invoked. So although there’s an .aliases file, those aliases are run directly..gitignoreSet files that you’d like Git to ignore on the entire system. Yay, no more .DS_Store being accidentally committed!.gvimrcA small file that improves readability for gvim..hgignoreSimliar to .gitignore for Mercurial..hushloginIn some instances, for example, when you ssh into a machine, you may be presented with a message. It might look something like this: | | _ __ ___  __ ___ ______ | |  ___  ___ _ _________ _ __| &#39;_ ` _ | | | |  / __/ _  / _ | | / __|/ _  &#39;__  / / _  &#39;__|| | | | | | |_| | | (_| (_) | (_) | | __   __/ | V /  __/ ||_| |_| |_|__, |  ______/ ___/|_| |___/___|_| _/ ___|_|__/ |  |___/Welcome to my cool server.Any malicious and/or unauthorized activity is strictly forbidden.All activity may be logged.This file prevents this from being shown..inputrcConfigures the ‘Readline environment’. This controls the way keys work when you’re entering a command into your shell.An example of how I find this useful is to make tab autocomplete regardless of filename case:set completion-ignore-case on.osxThis is my favorite of all the dotfiles. It is run once, manually, for the commands to run and take effect. Depending on what you’ve added to this file, you may need to restart your machine.Some of the awesome things I love are:  Disable the “Are you sure you want to open this application?” dialog  Check for software updates daily, not just once per week  Disable Notification Center and remove the menu bar icon  Enable access for assistive devices  Set a blazingly fast keyboard repeat rate  Finder: allow quitting via ⌘ + Q doing so will also hide desktop icons  When performing a search, search the current folder by default  Speed up Mission Control animations.screenrcIf you use screen, this removes the startup message..vimrcI’m not that familiar with vim. However some of the things you can do with this file include enabling line numbers and adding syntax highlighting.For vim i have also included the powerline visual styling which will include a status line.Important Notes  I haven’t included powerline in my main installation script, so if you wish to have it, then please proceed with installing it separately with the fonts dependency.  The --user parameter should be removed if you got an error while installation especially if you have python installed via Homebrew.  A dependency is the powerline fonts pack. Installation instructions can be found directly in the repository.after installing powerline enable it by adding to the .vimrc:set rtp+=/usr/local/lib/python2.7/site-packages/powerline/bindings/vimThis can be change depending on the path to the python directory.wgetrcIf you use wget, this adds additional settings such as changing the timeout to 60 seconds rather than the default 15 minutes. It also sets the retry to three, rather than the default 20!Getting StartedIf you notice, some files that have mentioned above don’t exist in this repo. This is because i am using the amazing bash-it repo to organize those dotfiles.I recommend you head overthere and read how to use them.dInstalling the ScriptsFor installing the above files outside of the workflow mentioned in the main configurations, you need to run the following command sh install.sh. The command will symlink all the files in this directory into your home directory, overwriting existing files.Sensible OS X defaultsWhen setting up a new Mac, you may want to set some sensible OS X defaults:./.osxInstall Homebrew formulaeWhen setting up a new Mac, you may want to install some common Homebrew formulae (after installing Homebrew, of course).brew bundle has been deprecated, thus i converted the files into executable shell ones:brew install ~/.brewfile.shyou can always remove or add files based on yor preferences. You can search for brew formulas here in order to add or get more information about the ones I am using.My file contains the following brew formulas:  Core Modules coreutils, moreutils, findutils, binutils  Bash, Bash plugins bash, bash-completion, zsh, grc  Development git, heroku, node, mercurial, iojs  Git speific addons git-extras, hub, bfg  Utilities htop-osx, id3tool, lesspipe, ssh-copy-id, the_silver_searcher, tree, fcrackzip, foremost, ack, p7zip, pigz, pv, gnu-sed, vim, wget  Network tools dns2tcp, knock, rename, webkit2png  Dupes grep, screenInstall native apps with brew caskYou could also install native apps with brew cask:brew install ~/.caskfile.shyou can always remove or add files based on yor preferences. You can search for cask formulas here in order to add or get more information about the ones I am using.My cask file contains the following applications:  Utilities path-finder, alfred, cakebrew, a-better-finder-rename, fastscripts, bettertouchtool, iterm2, launchrocket, gyazo, ubersicht, dockmod, liteicon, cleanmymac, bartender, flexiglass  Productivity todoist, evernote, mendeley-desktop, fantastical, slack, screenhero  Development sublime-text3, atom, sourcetree, github, brackets, coda, tower, transmission-remote-gui, macvim, sqlite-database-browser, lightpaper,  codebox, anvil, hex-fiend, ksdiff, codekit, MAMP  Internet, Networking firefox, google-chrome, charles, dropbox, google-drive, mailbox, viber, utorrent, plex-media-server  Misc vlc, imagealpha, imageoptim, steam, the-unarchiver, spotify, handbrake  Quick Look plugins betterzipql, qlcolorcode, qlmarkdown, qlprettypatch, qlstephen, quicklook-csv, quicklook-json, quicknfo, suspicious-package, webp-quicklook  Color pickers colorpicker-developer, colorpicker-skalacoloInstall NPM GlobalsThere are a bunch of useful Node.js command line tools that can be installed globally. For that, the file .npm_globals.sh define an array of those applications. My global npm packages are:  amok: A free open source, editor agnostic, cross-platform command line tool for fast incremental development, testing and debugging in web browsers  bower: Bower offers a generic, unopinionated solution to the problem of front-end package management, while exposing the package dependency model via an API that can be consumed by a more opinionated build stack  caniuse: Compatibility validation for support of HTML5, CSS3, SVG and more in desktop and mobile browsers  eslint: ESLint is a tool for identifying and reporting on patterns found in ECMAScript/JavaScript code  grunt: The JavaScript Task Runner  imageoptim: Node.js wrapper for some images compression algorithms  jscs: JavaScript Code Style  mocha: Simple, flexible, fun test fr**amework  nodemon: Nodemon will watch the files in the directory in which nodemon was started, and if any files change, nodemon will automatically restart your node application.  prettyjson: Package for formatting JSON data in a coloured YAML-style, perfect for CLI output  psi: PageSpeed Insights with reporting  should: Should is an expressive, readable, framework-agnostic assertion library. The main goals of this library are to be expressive and to be helpful. It keeps your test code clean, and your error messages helpful.  slap: Slap is a Sublime-like terminal-based text editor that strives to make editing from the terminal easier  sparkly: Generate sparklines  tmi: Find out the image weight in your pages, compare to the BigQuery quantiles and discover what images you can optimize further  vtop: A graphical activity monitor for the command lineReferencesTutorials  Execution sequence for .bash_profile, .bashrc, .bash_login, .profile and .bash_logout  Setting Up a Mac Dev Machine From Zero to Hero With DotfilesOther dotfiles repositories  @Mathias Bynens and his dotfiles repository  @ptb and his OS X Lion Setup repository  Ben Alman and his dotfiles repository  Chris Gerke and his tutorial on creating an OS X SOE master image + Insta repository  Cătălin Mariș and his dotfiles repository  Gianni Chiappetta for sharing his amazing collection of dotfiles  Jan Moesen and his ancient .bash_profile + shiny tilde repository  Lauri ‘Lri’ Ranta for sharing loads of hidden preferences  Matijs Brinkhuis and his dotfiles repository  Nicolas Gallagher and his dotfiles repository  Sindre Sorhus  Tom Ryder and his dotfiles repository  Kevin Suttle and his dotfiles repository and OSXDefaults project, which aims to provide better documentation for ~/.osx  Haralan Dobrev  Zach Holman dotfiles  YADR YADR is an opinionated dotfile repo that will make your heart sing  Eduardo Lundgren dotfiles which claims to be the first JavaScript-based dotfiles powered by Grunt.",
      "url"      : "/dotfiles-for-the-public/",
      "date"     : "2015-05-21 00:00:00 +0100"
    },
  
    {
      "title"    : "RUBIX",
      "category" : "Data Science",
      "content"  : "IntroductionCompanies have traditionally performed business analysis based on transactional data stored in legacy relational databases. The enterprise data available for decision makers was typically relationship management or enterprise resource planning data. However social media feeds, weblogs, sensor data, or data published by governments or international organizations are nowadays becoming increasingly available.  With today’s public data sets containing billions of data items, more and more companies are looking to integrate external data with their traditional enterprise data to improve business intelligence analysis. These distributed data sources however exhibit heterogeneous data formats and terminologies and may contain noisy data. RUBIX is a novel framework that enables business users to semi-automatically perform data integration on potentially noisy tabular data. This framework offers an extension to Open Refine (Formerly Google Refine) with novel schema matching algorithms leveraging Freebase rich types. First experiments show that using Linked Data to map cell values with instances and column headers with types improves significantly the quality of the matching results and therefore should lead to more informed decisions.  The quality and amount of structured knowledge available make it now feasible for companies to mine this huge amount of public data and integrate it in their next-generation enterprise information management systems. Analyzing this new type of data within the context of existing enterprise data should bring them new or more accurate business insights and allow better recognition of sales and market opportunities. These new distributed sources however raise tremendous challenges. They have inherently different file formats, access protocols or query languages. They possess their own data model with different ways of representing and storing the data.Data across these sources may be noisy (e.g. duplicate or inconsistent), uncertain or be semantically similar yet different. Integration and provision of a unified view for these heterogeneous and complex data structures therefore require powerful tools to map and organize the data. RUBIX is a framework that enables business users to semi-automatically combine potentially noisy data residing in heterogeneous silos. Semantically related data is identified and appropriate mappings are suggested to users. On user acceptance, data is aggregated and can be visualized directly or exported to Business Intelligence reporting tools.The framework is composed of a set of extensions to Open Refine (now called Open Refine) server and a plug-in to its user interface. Open Refine was selected for its extensibility as well as good cleansing and transformation capabilities. We first map cell values with instances and column headers with types from popular data sets from the Linked Open Data Cloud. To perform the matching, we use the Auto Mapping Core (also called AMC ) that combines the results of various similarity algorithms. The novelty of our approach resides in our exploitation of Linked Data to improve the schema matching process. We developed specific algorithms on rich types from vector algebra and statistics. The AMC generates a list of high-quality mappings from these algorithms allowing better data integration. First experiments show that Linked Data increases significantly the number of mappings suggested to the user. Schemas can also be discovered if column headers are not defined and can be improved when they are not named or typed correctly. Finally, data reconciliation can be performed regardless of data source languages or ambiguity. All these enhancements allow business users to get more valuable and higher-quality data and consequently to take more informed decisions.Related WorkWhile schema matching has always been an active research area in data integration, new challenges are faced today by the increasing size, number and complexity of data sources and their distribution over the network. Data sets are not always correctly typed or labeled and that hinders the matching process. In the past, some work has tried to improve existing data schemas  but literature mainly covers automatic or semi-automatic labeling of anonymous data sets through Web extraction. Examples include that automatically labels news articles with a tree structure analysis or that defines heuristics based on distance and alignment of a data value and its label.These approaches are however restricting label candidates to Web content from which the data was extracted goes a step further by launching speculative queries to standard Web search engines to enlarge the set of potential candidate labels. More recently, applies machine learning techniques to respectively annotate table rows as entities, columns as their types and pairs of columns as relationships, referring to the YAGO ontology. The work presented aims however at leveraging such annotations to assist semantic search queries construction and not at improving schema matching. With the emergence of the Semantic Web, new work in the area has tried to exploit Linked Data repositories.The authors of present techniques to automatically infer a semantic model on tabular data by getting top candidates from Wikitology and classifying them with the Google page ranking algorithm. Since the authors’ goal is to export the resulting table data as Linked Data and not to improve schema matching, some columns can be labeled incorrectly, and acronyms and languages are not well handled. In the Helix project, a tagging mechanism is used to add semantic information on tabular data. A sample of instances values for each column is taken and a set of tags with scores are gathered from online sources such as Freebase. Tags are then correlated to infer annotations for the column. The mechanism is quite similar to ours but the resulting tags for the column are independent of the existing column name and sampling might not always provide a representative population of the instance values.PropositionOpen Refine (formerly Google Refine and Freebase Gridworks) is a tool designed to quickly and efficiently process, clean and eventually enrich large amounts of data with existing knowledge bases such as Freebase. The tool has however some limitations: it was initially designed for data cleansing on only one data set at a time, with no possibility to compose columns from different data sets. Moreover, Open Refine has some strict assumptions over the input of spreadsheets which makes it difficult to identify primitive and complex data types. The AMC is a novel framework that supports the construction and execution of new matching components or algorithms. AMC contains several matching components that can be plugged and used, like string matchers (Levenshtein, JaroWinkler … etc.), data types matchers and path matchers. It also provides a set of combination and selection algorithms to produce optimized results (weighted average, average, sigmoid … etc.).Framework ArchitectureOpen Refine makes use of a modular web application framework similar to OSGi called Butterfly. The server-side written in Java maintains states of the data (undo/redo history, long-running processes, etc.) while the client-side implemented in Javascript maintains states of the user interface (facets and their selections, view pagination, etc.). Communication between the client and server is done through REST web services.Activity FlowThe data sets to match can be contained in files (e.g. csv, Excel spreadsheets, etc.) or defined in Open Refine projects. The inputs for the match module are the source and target files and/or projects that contain the data sets.  These projects are imported into the internal data structure (called schema) of the AMC .  The AMC then uses a set of built-in algorithms to calculate similarities between the source and target schemas on an element basis, i.e. column names in the case of spreadsheets or relational databases.  The output is a set of similarities, each containing a triple consisting of source schema element, target element, and similarity between the two.  These results are presented to the user in tabular form such that s/he can check, correct, and potentially complete the mappings.  Once the user has completed the matching of columns, the merge information is sent back to Open Refine, which calls the merge module.  This module creates a new project, which contains a union of the two projects where the matched columns of the target project are appended to the corresponding source columns.  The user can then select the columns that s/he wants to merge and visualize by dragging and dropping the required columns onto the fields that represent the x and y axes.  Once the selection has been performed, the aggregation module merges the filtered columns and the result can then be visualized.As aggregation operations can quickly become complex, our default aggregation module can be replaced by more advanced analytics on tabular data.Schema MatchingSchema matching is typically used in business to business integration, metamodel matching, as well as Extract, Transform, Load (ETL) processes. For non-IT specialists the typical way of comparing financial data from two different years or quarters, for example, would be to copy and paste the data from one Excel spreadsheet into another one, thus creating reduncancies and potentially introducing copy-and-paste errors. By using schema matching techniques it is possible to support this process semi-automatically, i.e. to determine which columns are similar and propose them to the user for integration. This integration can then be done with appropriate business intelligence tools to provide visualizations. One of the problems in performing the integration is the quality of data. The columns may contain data that is noisy or incorrect. There may also be no column headers to provide suitable information for matching. A number of approaches exploit the similarities of headers or similarities of types of column data. We propose a new approach that exploits semantic rich typing provided by popular datasets from the Linked Data cloud.Data ReconciliationReconciliation enables entity resolution, i.e. matching cells with corresponding typed entities in case of tabular data. Open Refine already supports reconciliation with Freebase but requires confirmation from the user. For medium to large data sets, this can be very time-consuming. To reconcile data, we therefore first identify the columns that are candidates for reconciliation by skipping the columns containing numerical values or dates. We then use the Freebase search API to query for each cell of the source and target columns the list of typed entities candidates. Results are cached in order to be retrieved by our similarity algorithms.Matching Unnamed and Untyped ColumnsThe AMC has the ability to combine the results of different matching algorithms. Its default built-in matching algorithms work on column headers and produce an overall similarity score between the compared schema elements. It has been proven that combining different algorithms greatly increases the quality of matching results. However, when headers are missing or ambiguous, the AMC can only exploit domain intersection and inclusion algorithms based on column data. We have therefore implemented three new similarity algorithms that leverage the rich types retrieved from Linked Data in order to enhance the matching results of unnamed or untyped columns.Cosine SimilarityThe first algorithm that we implemented is based on vector algebra. Let (v) be the vector of ranked candidate types returned by Freebase for each cell value of a column. Then: [v:=sum^K_{i=1}{a_i}*overrightarrow{t_i}] where (a_i) is the score of the entry and (overrightarrow{t_i}) is the type returned by Freebase. The vector notation is chosen to indicate that each distinct answer determines one dimension in the space of results. Each cell value now has a weighted result set that can be used for aggregation to produce a result vector for the whole column. The column result (V) is then given by: [V:sum^n_{i=1}{v_i}] We now compare the result vector of candidate types from the source column with the result vector of candidate types from the target column. Let (W) be the result vector for the target column, then the similarity (s) between the columns pair can be calculated using the absolute value of the cosine similarity function.Pearson Product-Moment Correlation Coefficient (PPMCC)The second algorithm that we implemented is PPMCC, a statistical measure of the linear independence between two variables (left(x,yright)) . In our method, x is an array that represents the total scores for the source column rich types, y is an array that represents the mapped values between the source and the target columns. The values present in x but not in y are represented by zeros.Spearman’s Rank Correlation CoefficientThe last algorithm that we implemented to match unnamed and untyped columns is Spearman’s rank correlation coefficient. It applies a rank transformation on the input data and computes PPMCC afterwards on the ranked data. In our experiments we used Natural Ranking with default strategies for handling ties and NaN values. The ranking algorithm is however configurable and can be enhanced by using more sophisticated measures.Handling Non-String ValuesSo far we have covered several methods to identify the similarity between “String” values, but how about other numeral values like dates, money, distance …etc. For this purpose we have implemented some basic type identifier that can recognize dates, money, numeral values, numerals used as identifiers. This will help us in better match corresponding entries. Adjusting AMC’s combination algorithms can be of great importance at this stage; for example assigning weights to different matchers and tweaking the configuration can result in more accurate results.ExperimentsWe present in this section results from experiments we conducted using the different methods described above. To appreciate the value of our approach, we have used a real life scenario that exposes common problems faced by the management in SAP. The data we have used come from two different SAP systems; the Event tracker and the Travel Expense Manager. The Event Tracker provides an overview of events (Conferences, Internal events … etc.) that SAP Research employees contribute to or host. The entries in this system contain as much information as necessary to give an overview of the activity like the Activity type and title, travel destination, travel costs divided into several sub categories (conference fees, accommodation, transportation and others), and duration related information (departure, return dates) … etc. Entries in the Event Tracker are generally entered in batches as employees fill in their planned events that they wish to attend or contribute to at the beginning of each year. Afterwards, managers according to their allocated budget can either accept or reject these planned events. On the other hand, the Travel Expense Manager contains the actual data for the successfully accepted events. This system is used by employees to enter their actual trip details in order to claim their expenses. It contains more detailed information and aggregated views of the events, such as the total cost, duration calculated in days, currency exchange rates and lots of internal system tags and Identifiers. Matching reports from these two systems is of great benefit to managers to organize and monitor their allocated budget, they mainly want to:  Find the number of the actual (accepted plans) compared with the total number of entered events.  Calculate the deviation between the estimated and actual cost of each event.However, matching from these two sources can face several difficulties that can be classified in two domains; the first is in global labels (or column headers as we are dealing with Excel like files). These problems can be summarized in:  Missing labels: Importing files into Open Refine with empty headers will result in assigning that column a dummy name by concatenating the word “column” with a number starting from 0.  Dummy labels or semantically unrelated names: This is a common problem especially from the data coming from the Travel Expense Manager. This can be applied to columns that are labeled according to the corresponding database table (i.e. lbl_dst to denote destination label). Moreover, column labels do not often convey the semantic type of the underlying data.The second domain is at cell (single entry) level; these problems can be summarized in:  Detecting different date formats: We have found out that dates field coming from the two systems have different formats, moreover; the built in type detection in Open Refine converts detected date (if found) into another third format.  Entries from different people can be made in different languages.  Entries in the two systems can be incomplete, an entry can be shortened automatically by the system; for example, selecting a country in the Travel Expense Manager will result in filling out that country code in the exported report (i.e. France = FR).  Inaccurate entries: This is one of the most common problems faced; users in the some field can enter several values that correspond to the same entity. For example, in the destination column, users can enter the country, the airport at the destination, the city or even the exact location of the event (i.e. office location).To continue reading the evaluation you can download the Full Paper Here",
      "url"      : "/rubix-a-framework-for-improving-data-integration-with-linked-data/",
      "date"     : "2015-04-13 00:00:00 +0100"
    },
  
    {
      "title"    : "Quora Better Notification (QBN)",
      "category" : "projects",
      "content"  : "  I have been really frustrated with the default Quora notifications page. Hundreds of new notifications everyday, lots of them are redundant (i.e. the same question added to several topics i follow or x number of people added an answer to a question i follow). And its not only me, i have recently read A typical day on Quora for me, or why notifications need to be burned before they lay eggs and that actually what sparked the urge in me to develop something in the weekend to solve not only what i thought my problem, but apparently a wide agreed upon one.  I have only found one attempt to fix this which is a bookmarklet that at the moment of writing this post was not functioning properly. However, some guy apparently ported that with some fixes into a Chrome extension called Quora Extender. Although it does a good job in combining the notifications, my comments are:  I don’t like the approach for the interface and packing of elements together  They don’t catch new notification fetched by the Quora infinite Scroll  They don’t provide any clustering for notifications  I cannot easily navigate through topics by any kind of filtersDownloadDiscussions about the issue  Improving Quora Notifications  Can Quora let me filter out board notifications from all the other ones?  Notifications on Quora  How could Quora improve notifications ?I have decided that this has to end, i want to better view my notifications. This extension is a simple one. It can identify several actions in the notifications feed: Quora Better Notifications (QBN) Chrome Extension  New question added to a topic you follow  Replies to a some action you posted  Comments on one of your posts  Comments on one of your answers  Suggested edit to question  Asked you to answer a question  Mentioned you in a post  Wrote an answer for a question you follow  Promoted a question you asked  You have a new follower to one for your blogs  You have a new follower to one of your questions  You have a new user following you  Voted up an answer for you on some question  Thanked you for an answer or contribution  Tweeted about some of your content  Suggested to edit your bioAfter Catching these notifications, i have hacked the sidebar and presented these as filters with a counter for the new ones. But doing only that is not enough for me at this point, i wanted to do some kind of clustering for some of the notification types.Notifications ClusteringAt the current version of the extension i only cluster two types of notifications:  New Answer Notification: Instead of having multiple entries for answers added to a question i follow, they are now clustered like User(X) and (N) others posted an answer to question you follow.  New Questions added to a Topic: It happens very often that a single question is added to multiple categories, they are now clustered like Question(X) has been added to (N) Topics : (Topic1, Topic2 … TopicN)Future WorkI am planning to finish clustering for all the other actions. For example, the upVoting notification will be clustered in a similar fashion to Twitter’s notification: User(X) and (N) others voted up your answer …Topics FilteringIf you are following lots of topics, you might be interested at some point to browse only a certain topic. For that, there is a dropdown list in the top that contains a list of all the topics (fine-grained list i.e. if there is a context found in a certain topic, the context is presented) discovered in the feed.Notes  Quora implements an infinite scroll mechanism to fetch new notifications. I had problems trying to catch when an infinite call is done as it is not done via a normal AJAX request as i could not intercept that by assigning a hook on the ajax.success() or ajax.complete() on the document. So at the moment, there is function that runs on intervals of 5 seconds that will check for new notifications fetched by Quora and then applies the clustering and cleaning.  This is a very early prototype that was during a weekend, so please try it out and i will appreciate any feedback  I will be keen on porting this plugin into a Bookmarklet if requestedDownload",
      "url"      : "/quora-better-notifications/",
      "date"     : "2014-05-01 00:00:00 +0100"
    },
  
    {
      "title"    : "Everything you need to know about RDF",
      "category" : "Semantic Web",
      "content"  : "Humans share, consume and produce knowledge using natural language; however presenting this knowledge in a machine readable and understandable format can be difficult. Lets take for example the fact that I (Ahmad) have (owner of ) a blog (http://ahmadassaf.com/blog). In natural language i can easily express that. If i want to convert this into XML which is one of the most popular data representation (markup) language, i can have the following representations:&amp;lt;!-- Presentation Method (1) --&amp;gt;&amp;lt;blog&amp;gt;     &amp;lt;owner&amp;gt;Ahmad&amp;lt;/owner&amp;gt;     &amp;lt;address&amp;gt;http://ahmadassaf.com/blog&amp;lt;/address&amp;gt;&amp;lt;/blog&amp;gt;&amp;lt;!-- Presentation Method  (2)--&amp;gt;&amp;lt;person name=&quot;Ahmad&quot;&amp;gt;     &amp;lt;blog&amp;gt;http://ahmadassaf.com/blog&amp;lt;/blog&amp;gt;&amp;lt;/person&amp;gt;&amp;lt;!-- Presentation Method  (3)--&amp;gt;&amp;lt;person name=&quot;Ahmad&quot; blog=&quot;http://ahmadassaf.com/blog&quot; /&amp;gt;However, this doesn’t come intuitively to humans as they are used to present statements generally in a systematic fashion of Subject - Verb/Predicate/Action - Object From this thought, the idea of presenting knowledge in a three parts form was the basis of the Resource Description Framework (RDF), where i can represent the following example saying that: This is an intuitive knowledge representation using directed graphs, where the subjects and objects are the nodes and the predicates are the edges of that graph. This statement that comprises of these three parts is called RDF-Triple where the resource is a URI or a blank (empty) node, the property is a URI and the object can be a URI, literal or a Blank Node. If we wish to transform this knowledge into the traditional relational model (tables) it will look like:SubjectPredicateObjectAhmadhas a bloghttp://ahmadassaf.com/blogConstituents of an RDF TripleURIsA Unique referenceable URI.  A URI reference is a Unicode string that does not contain any control characters and would produce a valid URI character sequence representing an absolute URI with optional fragment identifier when subjected to the encoding that consists of:  encoding the Unicode string as UTF-8 giving a sequence of octet values.  %-escaping octets that do not correspond to permitted US-ASCII characters.LiteralsSimple Strings that describe data values that do not have a separate existence. They can be plain (simple string combined with an optional language tag) or typed (string combined with a datatype URI and an optional language tag). Typed Literals are expressed via the XML Schema data types. Whenever we are using URIs to describe things in RDF we try as much as we can to reuse existing namespaces and for literals we use the XML Schema defined in http://www.w3.org/2001/XMLSchema#.  So for example if i want to define a literal as a String i use the following syntax :&quot;Semantics&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#String&amp;gt;We should note that literals are wrapped with quotations marks. The hash after the [highlight text=”XMLSchema ”] URI denotes the fragment identifier that points to String. In addition to these i can specify a language tag the describes the “natural” language of the text. For example, &quot;Semantic&quot;@en which means that this literal is an English world. Literals are written either using double-quotes when they do not contain linebreaks like &quot;simple literal&quot; or &quot;&quot;&quot;long literal&quot;&quot;&quot; when they may contain linebreaks. Datatypes are a bit tricky. Let’s think of the datatype for floating-point numbers. At an abstract level, the floating-point numbers themselves are different from the text we use to represent them on paper. For instance, the text “5.1” represents the number 5.1, but so does “5.1000” and “05.10”. Here there are multiple textual representations — what are called lexical representations — for the same value. A datatype tells us how to map lexical representations to values, and vice versa. RDF reuses the  [highlight text=”XML Schema (W3C)”] datatypes, including xsd:string, xsd:float, xsd:double, xsd:integer, and xsd:date RDF can also contain custom datatypes that (you guessed it!) are simply named with a URI. If you omit a datatype declaration it be considered as a plain literal by many RDF tools, which is not the same thing as a string. However, as of RDF 1.1 (still in development at the time of writing) this distinction is going away, so going forward you should be able to treat &quot;Rob Gonzalez&quot; and &quot;Rob Gonzalez^^xsd:string as equivalent, and many tools already do. The semantics of RDF takes language tags and datatypes into account. This means two things. First, a literal value without either a language tag or datatype is different from a literal with a language tag and is different from a literal with a datatype. These four statements say four different things and none can be inferred from the others:#ahmad foaf:name &quot;ahmad assaf&quot;                   ahmad&#39;s name is a lanaguage-less,datatype-less raw text value.#ahmad foaf:name &quot;ahmad assaf&quot;@en                ahmad&#39;s name, in English, is ahmad assaf.#ahmad foaf:name &quot;ahmed assef&quot;@fr                ahmad&#39;s name, in French, is ahmed assef.#ahmad foaf:name &quot;ahmad assaf&quot;^^xsd:string       ahmad&#39;s name is a string.So, an untyped literal with or without a language tag is not the same as a typed literal. The second part of the semantics of literals is that two typed literals that appear different may be the same if their datatype maps their lexical representations to the same value. The following statements are equivalent (at least for an RDF application that has been given the semantics of the XSD datatypes):#ahmad ex:age &quot;20&quot;^^xsd:float#ahmad ex:age &quot;20.000&quot;^^xsd:floatThese mean ahmad age is 20. That is, the textual representation of the number is besides the point and is not part of the meaning encoded by the triples. Note that if the float datatype were not specified, the triples would not be inherently equivalent, and the textual representation of the 20 would be maintained as part of the information content. Sometimes the value of a property needs to be a fragment of XML, or text that might contain XML markup. RDF/XML provides a special notation to make it easy to write literals of this kind. This is done using a third value of the rdf:parseType attribute. Giving an element the attribute rdf:parseType=&quot;Literal&quot; indicates that the contents of the element are to be interpreted as an XML fragment&amp;lt;rdf:Description rdf:ID=&quot;ahmadassaf&quot;&amp;gt;     &amp;lt;foaf:name rdf:parseType=&quot;Literal&quot;&amp;gt;          &amp;lt;span xml:lang=&quot;en&quot;&amp;gt;The &amp;lt;em&amp;gt;Owner&amp;lt;/em&amp;gt; of this Blog &amp;lt;/span&amp;gt;     &amp;lt;/foaf:name&amp;gt;&amp;lt;/rdf:Description&amp;gt;in Turtle notation the type used to describe XML literals is rdf:XMLLiteralBlank NodesSubjects or Objects can be modeled as blank nodes. They denote the existence of an individual with specific attributes but without providing any information about identity or reference. A Ground RDF Graph is a graph where there is no Blank Nodes .RDF RepresentationRDF Resources can be in principle anything that must be uniquely identified and referenced by a URI. The Description of sources is done via representing properties and relationships among sources; this representation can be done in several ways:Labeled Directed GraphThis is a visual way of modeling RD as a Node-Edge-Node Triple. Its directed as the direction of the edge is significant and always points towards the objectXML RDF NotationUsing XML syntax to represent RDF triples. We should point out that we do use namespaces in order to minimize the writing that we have to do. So, if we are using many URIs repeatedly throughout our representation then its better to define some global namespace and start referring to these namespaces with a shorthand.&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;&amp;gt;     &amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com&quot;&amp;gt;          &amp;lt;aa:hasBlog&amp;gt;               &amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot;&amp;gt;&amp;lt;/rdf:Description&amp;gt;          &amp;lt;/aa:hasBlog&amp;gt;     &amp;lt;/rdf:Description&amp;gt;&amp;lt;/rdf:RDF&amp;gt;we notice that every RDF triple is wrapped between the rdf:RDF tags. Defining namesapces is done using the xmlns (XML namespace) followed by : and the name of the namespace and then the URI. URIs are represented with the &amp;lt;rdf:description&amp;gt; tag and the URI for that RDF resource should be identified in the rdf:about attribute. In addition to that, custom properties are wrapped in tags named after the namesapce defined on top followed buy the path to the sepccific property &amp;lt;aa:hasBlog&amp;gt; We can also define more than one property for each resource:&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;&amp;gt;     &amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com&quot; aa:hasEmail=&quot;me@ahmadassaf.com&quot; &amp;gt;          &amp;lt;aa:hasBlog rdf:resource=&quot;http://ahmadassaf.com/blog&quot; /&amp;gt;          &amp;lt;aa:Name rdf:datatype=&quot;http://www.w3c.org/2001/XMLSchema#String&quot;&amp;gt;               Ahmad Assaf          &amp;lt;/aa:Name&amp;gt;     &amp;lt;/rdf:Description&amp;gt;&amp;lt;/rdf:RDF&amp;gt;In an RDF/XML document there are two types of XML nodes: 1) resource XML nodes and 2) property XML nodes. Resource XML nodes are the subjects and objects of statements, and they usually are rdf:Description tags that have an rdf:about attribute on them giving the URI of the resource they represent. In this example, the rdf:Description nodes are the resource nodes. Resource XML nodes contain within them property XML nodes (and nothing else). Each property XML node represents a single statement. The subject of the statement is the outer resource XML node that contains the property. We should also notice that we have used the rdf:datatype in order to specify the usage of certain types like Strings or Integers … etc. The URL can be shortcut by using namespaces as well. We have the xml:base attribute that defines a base URI that will be used globally in the XML representation. So for example, if i have identified a xml:base tag as follows:&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;xml:base=&quot;http://ahmadassaf.com/Administrators&quot;&amp;gt;and i have changed my URI from http://ahmadassaf.com to http://ahmadassaf.com/Administrators#AhmadAssaf then in the XML RDF i point out to that as:&amp;lt;rdf:Description rdf:about=&quot;#AhmadAssaf&quot; aa:hasEmail=&quot;...... &amp;gt;&amp;lt;/rdf:Description&amp;gt;instead of using rdf:about and then put the hash # in front of the fragment URI, i can use the rdf:ID which has the hash tag complemented in it so that i can write:&amp;lt;rdf:Description rdf:ID=&quot;AhmadAssaf&quot; aa:hasEmail=&quot;...... &amp;gt;&amp;lt;/rdf:Description&amp;gt;However, despite the fact that XML RDF is difficult to read and a bit expensive and not flexible, it is the standard for web documents as we can embed it simply as XML is supported by most browsers and parsers.N3 NotationSimple listing of triples. It is a shorthand non-XML serialization of RDF models designed with Human readability in mind. It is more compact than RDF XML but for complex and large models it can become very expensive.&amp;lt;http://ahmadassaf.com&amp;gt; &amp;lt;http://ahmadassaf.com/Personal#hasBlog&amp;gt; &amp;lt;http://ahmadassaf.com/blog&amp;gt;.N3 has some syntactic sugar that allows further abbreviations. If many statements repeat the same subject and predicate, just separate the objects with commas:&amp;lt;http://ahmadassaf.com&amp;gt; &amp;lt;http://ahmadassaf.com/Personal#hasBlog&amp;gt; &amp;lt;http://ahmadassaf.com/blog&amp;gt;, &amp;lt;http://MySecondBlog.com/&amp;gt;, &amp;lt;http://MyThirdBlog.com/&amp;gt; . And if the same subject is repeated, but with different predicates, one may use semicolons as in the example:&amp;lt;http://ahmadassaf.com&amp;gt; &amp;lt;http://ahmadassaf.com/Personal#hasBlog&amp;gt; &amp;lt;http://ahmadassaf.com/blog&amp;gt;; foaf:name &quot;Ahmad Assaf&quot; .Turtle (Terse RDF Triple Language)A simplified of the N3 notation. URIs are wrapped in angle brackets and Literals in quotations marks. Every triple ends up with a period and whitespaces or indentation will be ignored.&amp;lt;http://ahmadassaf.com&amp;gt;&amp;lt;http://ahmadassaf.com/Personal#hasBlog&amp;gt;&amp;lt;http://ahmadassaf.com/blog&amp;gt; . We have talked about namespaces and base URIs in XML. The same concepts are transfered into the turtle notation:@prefix xsd: &amp;lt;http://www.w3.org/2001/XMLSchema#&amp;gt;.@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt;.@prefix ex: &amp;lt;http://example.org/stuff/1.0/&amp;gt;.@base &amp;lt;http://ahmadassaf.com/Administrators&amp;gt;so in my example i will have::AhmadAssafaa:hasBlog &amp;lt;http://ahmadassaf.com/blog&amp;gt; ;ex:fullname &quot;Ahmad Assaf&quot;^^xsd.string .Note that we have used a semi colon to express that we want to keep stating facts about the same subject which is &amp;lt;http://ahmadassaf.com&amp;gt; We will go to more details about this notation a bit forward. In Turtle the referring to URIs in the base ID is done in a similar way of using the rdf:ID tag as the hash is complemented with the usage of colons : and the fragment URI ex: :AhmadAssaf The current base URI may be altered in a Turtle document using the @base directive. It allows further abbreviation of URIs but is usually for simplifying the URIs in the data, where the prefix directives are for vocabularies that describe the data. For example:# Here the scope base URI is the document URI&amp;lt;a1&amp;gt; &amp;lt;b1&amp;gt; &amp;lt;c1&amp;gt; .@base &amp;lt;http://example.org/ns/&amp;gt; .# In-scope base URI is http://example.org/ns/&amp;lt;a2&amp;gt; &amp;lt;http://example.org/ns/b2&amp;gt; &amp;lt;c2&amp;gt; .@base &amp;lt;foo/&amp;gt; .# In-scope base URI is http://example.org/ns/foo/ at this point ...Notes:  Turtle strings and URIs can use -escape sequences to represent Unicode code points (t, n, …. etc. )  Comments in Turtle take the form of #, outside a relative URI or strings, and continue to the end of line  The , symbol may be used to repeat the subject and predicate of triples that only differ in the object RDF term.# this is not a complete turtle document:a :b :c ,:d .# the last triple is :a :b :d .  The ; symbol may be used to repeat the subject of of triples that vary only in predicate and object RDF terms.# this is not a complete turtle document:a :b :c ;:d :e .# the last triple is :a :d :e .The Framework is a combination of web based protocols (URI, HTTP, XML … etc.) that are based on formal models and defines all the allowed relationships among resources.Blank Nodes (B Nodes )We have talked earlier about Blank Nodes or B Nodes. They are basically nodes that might not have names and are potentially un-referenceable. RDF only allows binary relations, so it’s necessary to express many-way relations using intermediate nodes, and these nodes are often anonymous.They are used usually to express collections, for example if i want to say that my blog is influenced by other resources, in natural language i can say that my blog at http://ahmadassaf.com/blog is influenced by Blog A at http://www.BlogA.com and Blog B at http://www.BlogB.com If i want to represent this as a directed graph i will do: The problem now is that how can i distinguish between these two sources ?! which URL corresponds to which resources name ?! so now we have to think of a workaround to ensure unique representation and selection of resources. We can make this happen (representing multi valued relation) by introducing additional nodes in the graph: The nodes in black are blank nodes that do not have a name and only act as a connection point ( A visual representation): Now how can we represent Blank Nodes in XML RDF and Turtle notation ?!&amp;lt;?xml version-&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;&amp;gt;&amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot; aa:hasName=&quot;Ahmad Assaf&#39;s Blog&quot; &amp;gt;     &amp;lt;aa:isInfluencedBy rdf:parseType=&quot;Resource&quot;/&amp;gt;          &amp;lt;aa:name&amp;gt;Blog A&amp;lt;/aa:name&amp;gt;          &amp;lt;aa:link&amp;gt;http://www.blogA.com&amp;lt;/aa:link&amp;gt;     &amp;lt;/aa:isInfluencedBy&amp;gt;&amp;lt;/rdf:Description&amp;gt;&amp;lt;/rdf:RDF&amp;gt;In XML RDF the blank nodes are denoted by the rdf:parseType=&quot;Resource&quot;, in Turtle its much more easier:@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .&amp;lt;http://ahmadassaf.com/blog&amp;gt; aa:isInfluencedBy [     aa:name &quot;Blog A&quot; ;     aa:link &quot;http://www.blogA.com&quot; ].we simply introduce blank nodes in Turtle by using the brackets [ ]. So far we have known that Blank Nodes do not have name, this is not always the case as sometime i need to give a label or a name to that blank node in order to be able to reference it locally. In this example the nodeID will follow the default base URI.&amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot; aa:hasName=&quot;Ahmad Assaf&#39;s Blog&quot;&amp;gt;     &amp;lt;aa:isInfluencedBy rdf:nodeID=&quot;Resouce1 /&amp;gt;&amp;lt;/rdf:Description&amp;gt;&amp;lt;rdf:Description rdf:nodeID=&quot;Resource1&quot;&amp;gt;     &amp;lt;aa:name&amp;gt;Blog A&amp;lt;/aa:name&amp;gt;     &amp;lt;aa:link&amp;gt;http://www.blogA.com&amp;lt;/aa:link&amp;gt;&amp;lt;/rdf:Description&amp;gt;In Turtle we use the underscore _ followed by a colon that denotes the default namespace and then the Blank Node name@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .&amp;lt;http://ahmadassaf.com/blog&amp;gt; aa:isInfluencedBy  _:Resource1 ._:Resource1 aa:name &quot;Blog A&quot; ;                aa:link &quot;http://www.blogA.com&quot; .so Blank Nodes do NOT need be de-referencable and accessed from the outside world, however they can have IDs or names that will them to be referenced in an RDF document or model.Data Structures in RDFIn RDF there exists some kind of collections (in computer science terms) that will allow us aggregate nodes or facts together. They are general data structures to enumerate any resource or literal. They are basically a syntactic sugar that will ease the process of writing code with no semantic expressiveness whatsoever. We have in RDF two different aggregators:ContainersAn open list of elements possibly including duplicate members where new entries (additions) are possible Note that the container resource (which may either be a blank node or a resource with a URIref) denotes the group as a whole. The members of the container can be described by defining a container membership property for each member with the container resource as its subject and the member as its object. These container membership properties have names of the form rdf:_n where n is a decimal integer greater than zero, with no leading zeros, e.g., rdf:_1, rdf:_2, rdf:_3 and so on, and are used specifically for describing the members of containers. Container resources may also have other properties that describe the container, in addition to the container membership properties and the rdf:type property. We notice that collection node has a type of rdf:type rdf:seq which means that this is a sequential collection or list and the order of the items is important. Each item in the list have a unique property rdf:_1, rdf:_2 ... etc. In Turtle, sequential collections are represented as:@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .@base &amp;lt;http://ahmadassaf.com/&amp;gt;:blog aa:hasAdmins [     a rdf:Seq;     rdf:_1 &amp;lt;A&amp;gt;;     rdf:_2 &amp;lt;B&amp;gt;;     rdf:_3 &amp;lt;C&amp;gt;.].Or it can be written as:@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .@base &amp;lt;http://ahmadassaf.com/&amp;gt;:blog aa:hasAdmins :administrators.:administrators rdf:type rdf:Seq.:administrators rdf:_1 &amp;lt;A&amp;gt;.:administrators rdf:_2 &amp;lt;B&amp;gt;.:administrators rdf:_3 &amp;lt;C&amp;gt;.Or written as::blog aa:hasAdmins :administrators.:administrators rdf:type rdf:Seq;rdf:_1 &amp;lt;A&amp;gt;;rdf:_2 &amp;lt;B&amp;gt;;rdf:_3 &amp;lt;C&amp;gt;.In XML/RDF&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;&amp;gt;&amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot;&amp;gt;&amp;lt;aa:hasAdmins&amp;gt;     &amp;lt;rdf:Bag&amp;gt;          &amp;lt;rdf:li rdf:resource=&quot;http://ahmadassaf.com/A&quot;/&amp;gt;          &amp;lt;rdf:li rdf:resource=&quot;http://ahmadassaf.com/B&quot;/&amp;gt;          &amp;lt;rdf:li rdf:resource=&quot;http://ahmadassaf.com/C&quot;/&amp;gt;     &amp;lt;/rdf:Bag&amp;gt;&amp;lt;/aa:hasAdmins&amp;gt;&amp;lt;/rdf:Description&amp;gt;&amp;lt;/rdf:RDF&amp;gt;RDF/XML provides rdf:li as a convenience element to avoid having to explicitly number each membership property. The numbered properties rdf:_1, rdf:_2 and so on are generated from the rdf:li elements in forming the corresponding graph. The element name rdf:li was chosen to be mnemonic with the term “list item” from HTML we notice that we referenced our blog by using the default namespace using the colon : and we have referred to the collection by the brackets [] (like Blank Nodes) but we have added the rdf:type rdf:Seq to that blank node. In RDF a short hand for the rdf:type is the letter a (like natural language in English when you say that this is a cat –&amp;gt; it means that this is sth with A type of cat ). We have known so far the Sequential container which denotes ordered list of elements, but we do also have another types of containers:  rdf:Bag  This is an unordered list of elements possibly including duplicate members and there is no given order for elements.  rdf:Alt Defines alternatives of elements and only one element of the given alternatives is relevant to the application ( a group of resources or literals that are alternatives (typically for a single value of a property) . An Alt container is intended to have at least one member, identified by the property rdf:_1. This member is intended to be considered as the default or preferred value. Other than the member identified as rdf:_1 the order of the remaining elements is not significant.CollectionsThese are closed lists where there is no extension possible. Elements of the list are already predefined. It really resembles the traditional list data structure with the fact that it is closed to insert operations. It is split recursively with Head (first) and Tail (rest), we end the list by linking to rdf:nil Each of the blank nodes forming this list structure is implicitly of type rdf:List that is, each of these nodes implicitly has an rdf:type property whose value is the predefined type rdf:List although this is not explicitly shown in the graph.@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .@base &amp;lt;http://ahmadassaf.com/&amp;gt;:swt blog aa:hasAdmins [     rdf:first &amp;lt;A&amp;gt;; rdf:rest [          rdf:first &amp;lt;B&amp;gt;; rdf:rest [               rdf:first &amp;lt;C&amp;gt;;               rdf:rest rdf:nil .]]].However, in Turtle there are shortcuts to ease the simplify the syntax. We do that by using a new type of brackets ( )@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .@base &amp;lt;http://ahmadassaf.com/&amp;gt;:blog aa:hasAdmins (&amp;lt;A&amp;gt; &amp;lt;B&amp;gt; &amp;lt;C&amp;gt; ) .The collection of RDF statements (RDF Triples) form together an RDF graph. The assertion of an RDF graph (assertion means that a term evaluates to true) amounts to the assertion of all triples in it, this means a conjunction (logical AND) between all the statement corresponding to all the triples in the graph.  The datastore that stores these triples is often called a triple storeRDF/XML provides a special notation to make it easy to describe collections using graphs of this form. In RDF/XML, a collection can be described by a property element that has the attribute rdf:parseType=&quot;Collection&quot;, and that contains a group of nested elements representing the members of the collection. RDF/XML provides the rdf:parseType attribute to indicate that the contents of an element are to be interpreted in a special way. In this case, the rdf:parseType=&quot;Collection&quot; attribute indicates that the enclosed elements are to be used to create the corresponding list structure in the RDF graph&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;&amp;gt;&amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot;&amp;gt;     &amp;lt;aa:hasAdmins rdf:parseType=&quot;collection&quot;&amp;gt;          &amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/A&quot; /&amp;gt;          &amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/B&quot; /&amp;gt;          &amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/C&quot; /&amp;gt;     &amp;lt;/aa:hasAdmins&amp;gt;&amp;lt;/rdf:Description&amp;gt;&amp;lt;/rdf:RDF&amp;gt;Semantic Web practitioners found it very difficult to deal with large amounts of triples for application development. There are lots of reasons that you would want to segment different subsets of triples from each other (simplified access control, simplified updating, trust, etc.), and vanilla RDF made segmentation tedious.Named GraphsAt first the community tried using reification to solve this data segmentation problem, but today everyone has converged on using named graphs. A Named Graph is a collection of RDF statement that are given an identifier (URI). When referring to triple in a name graph we often use a 4-tuple notation (often referred to as quad) instead of the standard 3-tuple one:&amp;lt;Named Graph&amp;gt;, &amp;lt;Subject&amp;gt;, &amp;lt;Predicate&amp;gt;, &amp;lt;Object&amp;gt;When using named graphs, TriG is the de facto serialization. It’s the same as Turtle except that statements in a single graph are grouped with {}@prefix rdf: &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt; .@prefix aa: &amp;lt;http://ahmadassaf.com/Personal#&amp;gt; .@base &amp;lt;http://ahmadassaf.com/&amp;gt;:blogGraph {     :blog aa:hasAdmins (&amp;lt;A&amp;gt; &amp;lt;B&amp;gt; &amp;lt;C&amp;gt; ) .}This trivial example puts all the statements in the document into a single named graph, egotistically called :blogGraph Again, like all things in RDF :blogGraph is a URI. Looking at the 4-tuples, it’s pretty obvious that the same statement can exist in multiple named graphs. This is by design and is a very important feature. By organizing the statement into named graphs, a Semantic Web application can implement access control, trust, data lineage, and other functionality very cleanly.RDF ReificationRDF applications sometimes need to describe other RDF statements using RDF, for instance, to record information about when statements were made, who made them, or other similar information (this is sometimes referred to as “provenance” information). Moreover, sometimes we will come across use cases where we deduce facts in our model, and the deducted facts also need to be used and modeled (become the subject of a new RDF statements). For example, if a detective (Sherlock Holmes) deduced that the gardener is the one who has killed the butler, then i might want to use this new discovered fact in a new RDF statement. This is another use case of reification.   Reification allows interleaving of RDF statements and making statements about other statementsRDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the  type rdf:Statement describes an RDF statement, consisting of the following properties: rdf:subject the described resourcerdf:predicate the original propertyrdf:object the value of the property  ex:Gardener ex:hasKilled ex:Butlerex:Sherlock ex:deduces ex: ????  The use of reification makes it fairly simple to represent this information:ex:Sherlock ex:deduces ex:StatementOnGardener .ex:statementOnGardener a rdf:Statement ;    rdf:subject ex:Gardener ;    rdf:predicate ex:hasKilled ;    rdf:object ex:butler .However, while RDF provides this reification vocabulary, care is needed in using it, because it is easy to imagine that the vocabulary defines some things that are not actually defined.for example, lets take the statement::blog aa:hasAdmins :ahmadassafUsing the reification vocabulary, a reification of the statement about the blog’s administrator would be given by assigning the statement a URIref such as :triple12345 (so statements can be written describing it), and then describing the statement using the statements::triple12345 rdf:type rdf:Statement.:triple12345 rdf:subject :blog.:triple12345 rdf:predicate aa:hasAdmin.:triple12345 rdf:object :ahmadassaf. &amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;              xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot;              xml:base=&quot;http://ahmadassaf.com/triples&quot;&amp;gt;&amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot;&amp;gt;     &amp;lt;aa:hasAdmins&amp;gt;          &amp;lt;rdf:Description rdf:ID=&quot;ahmadassaf&quot; /&amp;gt;     &amp;lt;/aa:hasAdmins&amp;gt;&amp;lt;/rdf:Description&amp;gt;  &amp;lt;rdf:Statement rdf:about=&quot;#triple12345&quot;&amp;gt;    &amp;lt;rdf:subject rdf:resource=&quot;http://ahmadassaf.com/blog&quot;/&amp;gt;    &amp;lt;rdf:predicate rdf:resource=&quot;http://ahmadassaf.com/Personal#hasAdmins&quot;/&amp;gt;    &amp;lt;rdf:object rdf:resource=&quot;http://ahmadassaf.com/ahmadassaf&quot; /&amp;gt;    &amp;lt;dc:creator rdf:resource=&quot;http://ahmadassaf.com/85740&quot;/&amp;gt;  &amp;lt;/rdf:Statement&amp;gt;&amp;lt;/rdf:RDF&amp;gt;The subject of these reification triples is a URI ref formed by concatenating the base URI of the document (given in the xml:base declaration), the character # (to indicate that what follows is a fragment identifier), and the value of the rdf:ID attribute. However you can generate the same graph (Reification) by using the rdf:ID&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;rdf:RDF xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;              xmlns:aa=&quot;http://ahmadassaf.com/Personal#&quot;              xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot;              xml:base=&quot;http://ahmadassaf.com/triples&quot;&amp;gt;&amp;lt;rdf:Description rdf:about=&quot;http://ahmadassaf.com/blog&quot;&amp;gt;     &amp;lt;aa:hasAdmins rdf:ID=&quot;triple12345&quot; &amp;gt;          &amp;lt;rdf:Description rdf:ID=&quot;ahmadassaf&quot; /&amp;gt;     &amp;lt;/aa:hasAdmins&amp;gt;&amp;lt;/rdf:Description&amp;gt;  &amp;lt;rdf:Description rdf:about=&quot;#triple12345&quot;&amp;gt;    &amp;lt;dc:creator rdf:resource=&quot;http://ahmadassaf.com/85740&quot;/&amp;gt;  &amp;lt;/rdf:Description &amp;gt;&amp;lt;/rdf:RDF&amp;gt;In this case, specifying the attribute rdf:ID=&quot;triple12345&quot; in the aa:hasAdmins element results in the original triple describing the blog administrator:http://ahmadassaf/blog aa:hasAdmins http://ahmadassaf.com/triples#ahmadassafplus the reification triples:http://ahmadassaf.com/triples#triple12345 rdf:type rdf:Statement.http://ahmadassaf.com/triples#triple12345 rdf:subject :blog.http://ahmadassaf.com/triples#triple12345 rdf:predicate aa:hasAdmin.http://ahmadassaf.com/triples#triple12345 rdf:object :ahmadassaf.Reification Advantages  Modeling data provenance  Formalizing statements about Reliability and Trust  Definition of metadata about statements (Assertions and Statements)Wrap Up  An RDF Model is a set of RDF statements  An RDF statement consists of subject, property, object  Subjects and objects and resources while an object can be either a literal or a resourceSo far we have talked about several RDF representations (A serialization format is a way to encode information so that when it’s passed between machines it can be parsed, XML is a serialization format, RDF is a data model); however there are few more worth mentioning:  TriG: TriG is Turtle but with support for named graphs. It’s the de facto standard for serializing RDF with named graphs.  RDFa (RDF embedded in HTML): You can embed RDF data within normal web pages by using RDFa.  N-Triples: is a very basic RDF serialization. Its key feature is that only one triple exists per line so that it’s very quick to parse and so that Unix command-line tools can easily manipulate it. It’s also highly compressible, so large, public RDF sources like DBpedia often publish data in N-Triples form.What sets RDF apart from XML is that RDF is designed to represent knowledge in a distributed world. That RDF is designed for knowledge, and not data, means RDF is particularly concerned with meaning. Everything at all mentioned in RDF means something. It may be a reference to something in the world, like a person or movie, or it may be an abstract concept, like the state of being friends with someone else. And by putting three such entities together, the RDF standard says how to arrive at a fact. The second key aspect of RDF is that it works well for distributed information. That is, RDF applications can put together RDF files posted by different people around the Internet and easily learn from them new things that no single document asserted. It does this in two ways, first by linking documents together by the common vocabularies they use, and second by allowing any document to use any vocabulary. This allows enormous flexibility in expressing facts about a wide range of things, drawing on information from a wide range of sources.  To distinguish between URIs, namespaced names (abbreviated URIs), anonymous nodes, and literal values, I used the following common convention  Full URIs are enclosed in angle brackets.  Namespaced names are written plainly, but their colons give them away.  Anonymous nodes are written like namespaced names, but in the reserved “_” namespace with an arbitrary local name after the colon.  Literal values are enclosed in quotation marks.",
      "url"      : "/everything-you-need-to-know-about-rdf/",
      "date"     : "2014-02-22 00:00:00 +0000"
    },
  
    {
      "title"    : "Cleaning up Wordpress tags",
      "category" : "web development",
      "content"  : "During the process of designing my blog (which i considered as a learning playground mostly) i had to import/export the database multiple times, delete and review posts and do lots of changes in my posts. In the end, i have noticed that although i have cleaned my posts, the number of tags is still unrealistic. After some digging, i discovered that at some point, WordPress was not cleaning my tags for unknown reasons to me up to now. I have decided to write a small PHP script that will scan my WordPress database and posts to determine the unique tags that i should ONLY have. The code below will do the following steps:  Retrieve all published posts (the status can be changed easily in the passed param array)  Get all the tags in WordPress database  Get all the tags assigned to these posts  Check if there exist any WordPress post with no tags assigned and notify the user  Print out the statistics, and delete all the extra tags found (you can enter a prompt here or allow for manual deletion)&amp;lt;?php// Include Wordpress main file to be able to access WP functionsinclude(&#39;blog/wp-load.php&#39;);// Define the scope of the search for posts, pages (published, drafts ... )$args = array( &#39;post_type&#39; =&amp;gt; &#39;post&#39;, &#39;post_status&#39; =&amp;gt; &#39;publish&#39;, &#39;posts_per_page&#39; =&amp;gt; -1);$posts_array = get_posts( $args );echo &quot; We have found &quot;. count($posts_array) . &quot; posts&amp;lt;/br&amp;gt;&amp;lt;/br&amp;gt;&quot;;// The array that will hold the total tags for all the posts$tags_array = array();// The tags array that is returned by Wordpress that contains extra bad tags$tags = get_tags();// This array will only hold the names of the tags in order to create a diff$tagsNames = array();$wordpress_total_tags = count($tags);// Start the WP loop in order to check postsforeach( $posts_array as $post ) { setup_postdata($post); ob_start();ob_end_clean(); $post_tags = get_the_tags(); //get the post tags // Check if the post contain any tags, if not we will notify the user with the post&#39;s name and link if ($post_tags) {  foreach($post_tags as $tag) {$tags_array[] =  $tag-&amp;gt;term_id ; //get the tag name. This can be changed to term_id or whatever you want  } } else echo &quot;We have found no Tags in post: &quot; . $post-&amp;gt;ID . &quot; name: &quot;. $post-&amp;gt;post_name .&quot; &amp;lt;a href=&#39;http://ahmadassaf.com/blog/wp-admin/post.php?post=&quot; . $post-&amp;gt;ID.&quot;&amp;amp;action=edit&#39;&amp;gt;Link&amp;lt;/a&amp;gt;&amp;lt;/br&amp;gt;&amp;lt;/br&amp;gt;&quot;;}$total_posts_tags= count($tags_array); //total number of tags in all posts$unique_tags_array  = array_unique($tags_array); //unique tags only from the posts$actual_unique_tags = count($unique_tags_array); //the number of unique tags foundecho &quot;Total of: &quot; . $wordpress_total_tags . &quot; Tags in Wordpress Posts&amp;lt;/br&amp;gt;&quot;;echo &quot;Total of: &quot; . $total_posts_tags . &quot; REAL Tags in Wordpress Posts&amp;lt;/br&amp;gt;&quot;;echo &quot;Total of: &quot; . $actual_unique_tags . &quot; REAL &amp;amp; UNIQUE Tags in Wordpress Posts&amp;lt;/br&amp;gt;&amp;lt;/br&amp;gt;&quot;;echo &quot;There are: &quot; . ($wordpress_total_tags - $actual_unique_tags) . &quot; Extra Tags Found&amp;lt;/br&amp;gt;&quot;;foreach ($tags as $tag) { $tagsNames[] = $tag-&amp;gt;term_id ;}$extra_tags =  array_diff($tagsNames, $unique_tags_array);foreach ($extra_tags as $term_id) { wp_delete_term( $term_id, &#39;post_tag&#39;);}?&amp;gt;The end result will look like this:We have found 285 postsWe have found no Tags in post: 60231 name: modern-powerful-php-debugging-helper-kint LinkTotal of: 1009 Tags in Wordpress PostsTotal of: 1686 REAL Tags in Wordpress PostsTotal of: 788 REAL &amp;amp; UNIQUE Tags in Wordpress PostsThere are: 221 Extra Tags FoundArray ( [1] =&amp;gt; .htaccess [3] =&amp;gt; 960 Grid System [9] =&amp;gt; Add-ons [11] =&amp;gt; Adipoli [12] =&amp;gt; Adobe Creative Suite [13] ........Running the code again will outputWe have found 285 postsWe have found no Tags in post: 60231 name: modern-powerful-php-debugging-helper-kint LinkTotal of: 788 Tags in Wordpress PostsTotal of: 1686 REAL Tags in Wordpress PostsTotal of: 788 REAL &amp;amp; UNIQUE Tags in Wordpress PostsThere are: 0 Extra Tags FoundI hope you found this post useful, and please don’t hesitate to write a comment if you have any question.",
      "url"      : "/cleaning-wordpress-tags-the-right-way/",
      "date"     : "2014-02-18 00:00:00 +0000"
    },
  
    {
      "title"    : "Introduction to Semantic Web",
      "category" : "Semantic Web",
      "content"  : "The model behind the Web could be roughly summarized as a way to publish documents represented in a standard way (HTML), containing links to other documents and accessible through the Internet using standard protocols (TCP/IP and HTTP). The result could be seen as a worldwide, distributed file system of interconnected documents that humans can read, exchange and discuss.  Before the web, people used to write documents, cite references and then check the reference and go and look search for it, in the library or look in the library … etc.  The great invention of the web is the hyperlink; click on that link and you get to the next document in the chain .. you can easily go to the reference !! so the web 1.0 was the web of documents  Web 2.0 was application silos .. social stuff .. its not nly about the data .. by the problem is that these systems do not interoperate (update facebook profile doesnt affect you linkedin ) .. data are not linked -&amp;gt; this was not only in web but also inside enterprise data  Web 3.0 is all about connecting the data .. not the documents but the data at lower levelsIn summary, the great advantage of Web 1.0 was that it abstracted away the physical storage and networking layers involved in information exchange between two machines. This breakthrough enabled documents to appear to be directly connected to one another. Click a link and you’re there—even if that link goes to a different document on a different machine on another network on another continent! In the same way that Web 1.0 abstracted away the network and physical layers, the Semantic Web abstracts away the document and application layers involved in the exchange of information.The Semantic Web connects facts, so that rather than linking to a specific document or application, you can instead refer to a specific piece of information contained in that document or application. If that information is ever updated, you can automatically take advantage of the update. The word semantic itself implies meaning or understanding. As such, the fundamental difference between Semantic Web technologies and other technologies related to data (such as relational databases or the World Wide Web itself) is that the Semantic Web is concerned with the meaning and not the structure of data. This fundamental difference engenders a completely different outlook on how storing, querying, and displaying information might be approached. Some applications, such as those that refer to a large amount of data from many different sources, benefit enormously from this feature.  Others, such as the storage of high volumes of highly structured transactional data, do not.What is meant by “semantic” in the Semantic Web is not that computers are going to understand the meaning of anything, but that the logical pieces of meaning can be mechanically manipulated by a machine to useful ends. So now imagine a new Web where the real content can be manipulated by computers. For now, picture it as a web of databases. One “semantic” website publishes a database about a product line, with products and descriptions, while another publishes a database of product reviews. A third site for a retailer publishes a database of products in stock. What standards would make it easier to write an application to mesh distributed databases together, so that a computer could use the three data sources together to help an end-user make better purchasing decisions? Semantic Web itself does not deal with unstructured content; instead, it is about representing not only structured data and links but also the meaning of the underlying concepts and relationships There’s nothing stopping anyone from writing a program now to do those sorts of things, in just the same way that nothing stopped anyone from exchanging data before we had XML. But standards facilitate building applications, especially in a decentralized system.  From a technical point of view, the Semantic Web consists of:  Data Model : RDF (Resource Description Framework): The data modeling language for the Semantic Web. All Semantic Web information is stored and represented in the RDF. It is a flexible and abstract model meaning that there is more than one representation of RDF.  Query Language: SPARQL (SPARQL Protocol and RDF Query Language): The query language of the Semantic Web. It is specifically designed to query data across various systems.  Schema and Ontology Languages: OWL (Web Ontology Language) - RDF Schema (RDFS) The schema language, or knowledge representation (KR) language, of the Semantic Web. They enable you to define concepts composably so that these concepts can be reused as much and as often as possible. Composability means that each concept is carefully defined so that it can be selected and assembled in various combinations with other concepts as needed for many different applications and purposes.The term semantic technologies represents a fairly diverse family of technologies that have been in existence for a long time and seek to help derive meaning from information. Some examples of semantic technologies include natural language processing (NLP), data mining, artificial intelligence (AI), category tagging, and semantic search. You might think of the goal of semantic technologies as separating signal from noise. Some examples of existing semantic technologies being used today include:  Natural-language processing (NLP):  NLP technologies attempt to process unstructured text content and extract the names, dates, organizations, events, etc. that are talked about within the text. There are many extensions of NLP and they include: Search: Semantic Search often requires NLP parsing of source documents. The specific technique used is called Entity Extraction, which basically identifies proper nouns (e.g., people, places, companies) and other specific information for the purposes of searching. For example, consider the query, “Find me all documents that mention Barack Obama.” Some documents might contain “Barack Obama,” others “President Obama,” and still others “Senator Obama.” When used correctly, extractors will map all of these terms to a single concept.Auto-categorization: Imagine that you have 100,000 news articles and you want to sort them based on certain specific criteria. That would take a human ages to do, but a computer can do it very quickly.Sentiment Analysis: Sentiment Analysis measures the “sentiment” of an article, typically meaning whether the article’s tone is positive, negative, or neutral. This application of NLP technology is often used in conjunction with search, but it can also be used in other contexts, such as alerting. For example, a business owner might ask an application to “alert me when someone says something negative regarding my company on Facebook.”Summarization: Often used in conjunction with research applications, summaries of topics are created automatically so that actual people do not have to wade through a large number of long-winded articles (perhaps such as this one!).Question Answering: This is the new hot topic in NLP, as evidenced by Siri and Watson. However, long before these tools, we had Ask Jeeves (now Ask.com), and later Wolfram Alpha, which specialized in question answering. The idea here is that you can ask a computer a question and have it answer you (Star Trek-style! “Computer…”).   Data mining: Data mining technologies employ pattern-matching algorithms to tease out trends and correlations within large sets of data. Data mining can be used, for example, to identify suspicious and potentially fraudulent trading behavior in large databases of financial transactions.  Artificial intelligence or expert systems: AI or expert systems technologies use elaborate reasoning models to answer complex questions automatically. These systems often include machine-learning algorithms that can improve the system’s decision-making capabilities over time.  Classification: Classification technologies use heuristics and rules to tag data with categories to help with searching and with analyzing information.  Semantic search: Semantic search technologies allow people to locate information by concept instead of by keyword or key phrase. With semantic search, people can easily distinguish between searching for John F. Kennedy, the airport, and John F. Kennedy, the president.The main goal behind knowing these technologies is that they help us in assembling the building blocks of the Semantic Web. For example, NLP can be used to extract structured data from unstructured documents (flat files like text documents). This data is then linked via Semantic Web technologies to other published data. This bridges the gap between documents (unstructured data) and structured data.Linked DataOne of the most important movements in the Semantic Web community is Linked Data, which strives to expose and connect all of the world’s data in a readily queryable and consumable form. The goal of Linked Data is to publish structured data in such a way that it can be easily consumed and combined with other Linked Data. The Four Rules of Linked DataSo in a way, Linked Data is the Semantic Web realized via four best practice principles.  Use URIs as names for things. An example of a URI is any URL. For example: http://ahmadassaf.com is the URI that refers to Ahmad Assaf.  Use HTTP URIs so that people can look up those names.  When someone looks up a URI, provide useful information, using the standards such as RDF and SPARQL.  Include links to other URIs so that they can discover more things. The Four Rules Applied  Instead of using application-specific identifiers—database keys, UUIDs, incremental numbers, etc.—you map them to a set of URIs. Each identifier must map to one single URI. For example, each row of those two tables is now uniquely identifiable using its URI.  Make your URIs dereferenceable. This means, roughly, to make them accessible via HTTP as we do for every human-readable Web page. This is a key aspect of Linked Data: every single row of our tables is now fetch able and uniquely identifiable anywhere on the Web.  Have our web server reply with some structured data when invoked. This is the Semantic Web “juicy” part. Model your data with RDF. Here is where you need to perform a paradigm shift from a relational data model to a graph one.  Once all the rows of our tables have been uniquely identified, made dereferenceable through HTTP, and described with RDF, the last step is providing links between different rows across different tables. The main aim here is to make explicit those links that were implicit before shifting to the Linked data approach.",
      "url"      : "/introduction-to-semantic-web/",
      "date"     : "2014-01-21 00:00:00 +0000"
    },
  
    {
      "title"    : "Automatic Wordpress Thumbnails",
      "category" : "web development",
      "content"  : "It happens very often that you design your WordPress theme without a need to utilize thumbnails. But, what happens if you eventually need them?! The effort of manually assigning posts thumbnails is huge. In this post, we will examine how we can extract the first image of a post, add that image to the WordPress media library and finally assign that image to be the default thumbnail. I did run this code in a separate PHP file in the root of my domain. You need to provide access to your WordPress installation though. In the first step, we first need to fetch all the posts that wee need to inspect. In my snippet i specified that i need to search for posts that have been published. You can change these parameters to include drafts, pending posts, pages and so on.&amp;lt;?php $args = array( &#39;post_type&#39; =&amp;gt; &#39;post&#39;, &#39;post_status&#39; =&amp;gt; &#39;publish&#39;, &#39;posts_per_page&#39; =&amp;gt; -1); $posts_array = get_posts( $args ); echo &quot; We have found &quot;. count($posts_array) . &quot; posts&amp;lt;/br&amp;gt;&quot;;?&amp;gt;Now, we need to loop through all the posts and start searching. To look for all images in a post we perform a preg_match_all on the post’s content.&amp;lt;?php foreach( $posts_array as $post ) {  setup_postdata($post);  // Print out the title of the post being examined  echo $post-&amp;gt;post_title.&quot;n&quot;;  $first_img = &#39;&#39;;  ob_start();ob_end_clean();  $output = preg_match_all(&#39;/&amp;lt;img.+src=[&#39;&quot;]([^&#39;&quot;]+)[&#39;&quot;].*&amp;gt;/i&#39;, $post-&amp;gt;post_content, $matches);  $first_img = $matches [1] [0];  // Check if image is on the same domain  $isSameDomain = !strpos($first_img,&#39;ahmadassaf.com&#39;) &amp;amp;&amp;amp; $first_img);  // Check if an image was found, you can add the boolean of the first domain &amp;amp;&amp;amp; $isSameDomain  if ($first_img) {$attachment_id = get_attachment_id_from_src($first_img);set_post_thumbnail( $post-&amp;gt;ID ,$attachment_id );echo set_featured_image($post-&amp;gt;ID, &#39;medium&#39;).&quot;&quot;; } }?&amp;gt;Set the Featured Image&amp;lt;?php function set_featured_image($post_id,$filename) {  $wp_filetype = wp_check_filetype(basename($filename), null );  $attachment = array(&#39;post_mime_type&#39; =&amp;gt; $wp_filetype[&#39;type&#39;],&#39;post_title&#39; =&amp;gt; preg_replace(&#39;/.[^.]+$/&#39;, &#39;&#39;, basename($filename)),&#39;post_content&#39; =&amp;gt; &#39;&#39;,&#39;post_status&#39; =&amp;gt; &#39;inherit&#39;);  $attach_id = wp_insert_attachment( $attachment, $filename, $post_id );  // you must first include the image.php file  // for the function wp_generate_attachment_metadata() to work  require_once(ABSPATH . &quot;wp-admin&quot; . &#39;/includes/image.php&#39;);  $attach_data = wp_generate_attachment_metadata( $attach_id, $filename );  if (wp_update_attachment_metadata( $attach_id,  $attach_data )) {// set as featured imagereturn update_post_meta($post_id, &#39;_thumbnail_id&#39;, $attach_id);  }  echo &quot;Featured Image Set (Y)&quot;;}?&amp;gt;Get the Attachment ID&amp;lt;?php function get_attachment_id_from_src ($image_src) {  global $wpdb;  $query = &quot;SELECT ID FROM {$wpdb-&amp;gt;posts} WHERE guid=&#39;$image_src&#39;&quot;;  $id = $wpdb-&amp;gt;get_var($query);  return $id; }?&amp;gt;The Code&amp;lt;?php // Include Wordpress main file to be able to access WP functions include(&#39;blog/wp-load.php&#39;); // Define the scope of the search for posts, pages (published, drafts ... ) $args = array( &#39;post_type&#39; =&amp;gt; &#39;post&#39;, &#39;post_status&#39; =&amp;gt; &#39;pending&#39;, &#39;posts_per_page&#39; =&amp;gt; -1); $posts_array = get_posts( $args ); echo &quot; We have found &quot;. count($posts_array) . &quot; posts&quot;; foreach( $posts_array as $post ) {  setup_postdata($post);  // Print out the title of the post being examined  echo $post-&amp;gt;post_title.&quot;n&quot;;  $first_img = &#39;&#39;;  ob_start();ob_end_clean();  $output = preg_match_all(&#39;/&amp;lt;img.+src=[&#39;&quot;]([^&#39;&quot;]+)[&#39;&quot;].*&amp;gt;/i&#39;, $post-&amp;gt;post_content, $matches);  $first_img = $matches [1] [0];  // Check if image is on the same domain  $isSameDomain = !strpos($first_img,&#39;ahmadassaf.com&#39;) &amp;amp;&amp;amp; $first_img);  // Check if an image was found, you can add the boolean of the first domain &amp;amp;&amp;amp; $isSameDomain  if ($first_img) {$attachment_id = get_attachment_id_from_src($first_img);set_post_thumbnail( $post-&amp;gt;ID ,$attachment_id );echo set_featured_image($post-&amp;gt;ID, &#39;medium&#39;).&quot;&quot;; } } function set_featured_image($post_id,$filename) {  $wp_filetype = wp_check_filetype(basename($filename), null );  $attachment = array(&#39;post_mime_type&#39; =&amp;gt; $wp_filetype[&#39;type&#39;],&#39;post_title&#39; =&amp;gt; preg_replace(&#39;/.[^.]+$/&#39;, &#39;&#39;, basename($filename)),&#39;post_content&#39; =&amp;gt; &#39;&#39;,&#39;post_status&#39; =&amp;gt; &#39;inherit&#39;);  $attach_id = wp_insert_attachment( $attachment, $filename, $post_id );  // you must first include the image.php file  // for the function wp_generate_attachment_metadata() to work  require_once(ABSPATH . &quot;wp-admin&quot; . &#39;/includes/image.php&#39;);  $attach_data = wp_generate_attachment_metadata( $attach_id, $filename );  if (wp_update_attachment_metadata( $attach_id,  $attach_data )) {// set as featured imagereturn update_post_meta($post_id, &#39;_thumbnail_id&#39;, $attach_id);  }  echo &quot;Featured Image Set (Y)&quot;;} function get_attachment_id_from_src ($image_src) {  global $wpdb;  $query = &quot;SELECT ID FROM {$wpdb-&amp;gt;posts} WHERE guid=&#39;$image_src&#39;&quot;;  $id = $wpdb-&amp;gt;get_var($query);  return $id; }?&amp;gt;",
      "url"      : "/how-to-extract-a-post-first-image,-add-it-to-the-media-library-and-assign-it-as-a-post-thumbnail-in-wordpress/",
      "date"     : "2013-10-23 00:00:00 +0100"
    },
  
    {
      "title"    : "Search Wordpress Posts",
      "category" : "web development",
      "content"  : "During my many trials to start a useful blog, i have tried a handful of plugins and extensions that do all sort of stuff. However, after a while, especially when i became more and more familiar with PHP and WordPress, i decided to dump lots of these plugins and go with self developed tailored functions. Unfortunately, uninstalling these plugins did not solve all my problems. Some of them did actually insert some special markup to my posts ( mostly logos and links, for example the Zemanta Plugin ). I decided that i had to clean up my posts from such ‘malicious’ markup. So i had to first of all to find the infected posts. To do so, i did write a small snippet that will run and search for several posts that contain specific strings, or characters or codes. I did run this code in a separate PHP file in the root of my domain. You need to provide access to your WordPress installation though. In the first step, we first need to fetch all the posts that wee need to inspect. In my snippet i specified that i need to search for posts that have been published. You can change these parameters to include drafts, pending posts, pages and so on.&amp;lt;?php $args = array( &#39;post_type&#39; =&amp;gt; &#39;post&#39;, &#39;post_status&#39; =&amp;gt; &#39;publish&#39;, &#39;posts_per_page&#39; =&amp;gt; -1); $posts_array = get_posts( $args ); echo &quot; We have found &quot;. count($posts_array) . &quot; posts&amp;lt;/br&amp;gt;&quot;;?&amp;gt;Now, we need to loop through all the posts and start searching&amp;lt;?phpforeach( $posts_array as $post ) {    setup_postdata($post);    ob_start();ob_end_clean();    $output = strstr($post-&amp;gt;post_content, &#39;zemanta-pixie&#39;);    $post_categories = wp_get_post_categories( $post-&amp;gt;ID );    $output =  in_category( &#39;miscellaneous&#39; ) &amp;amp;&amp;amp; count($post_categories) == 1;    if ($output) {        echo &quot;We have found code in post: &quot; . $post-&amp;gt;ID . &quot; name: &quot;.       $post-&amp;gt;post_name .&quot; &amp;lt;a href=YOUR-WORDPRESS-ADDRESS/wp-admin/post.php?post=&quot; .       $post-&amp;gt;ID.&quot;&amp;amp;action=edit&#39;&amp;gt;Link&amp;lt;/a&amp;gt;&amp;lt;/br&amp;gt;&quot;;    }}?&amp;gt;The search part happens in the string matching, you can change the string to whatever you want to look for:```php&amp;lt;?php $output = strstr($post-&amp;gt;post_content, &#39;zemanta-pixie&#39;);?&amp;gt;You can perform a set of secondary sanity checks if you wish, i have included some just in case. For example, i am checking if the post found is not a specific category “miscellaneous” and that that the post belongs to only ONE category.&amp;lt;?php $post_categories = wp_get_post_categories( $post-&amp;gt;ID ); $output =  in_category( &#39;miscellaneous&#39; ) &amp;amp;&amp;amp; count($post_categories) == 1;?&amp;gt;Finally, when we output the link you have to fill in the corresponding WordPress Installation URL so that you will be able to directly edit the post.&amp;lt;a href=YOUR-WORDPRESS-ADDRESS/wp-admin/post.php?post=&quot; . $post-&amp;gt;ID.&quot;&amp;amp;action=edit&#39;&amp;gt;Link&amp;lt;/a&amp;gt;Now, if you are searching for specific HTML tags in your post then you only need to change the search line to become:&amp;lt;?php$output = preg_match_all(&#39;/&amp;lt;blockquote.*&amp;gt;/&#39;, $post-&amp;gt;post_content, $matches);?&amp;gt;where we look for the existence of blockquote tags.The code&amp;lt;?php // Include Wordpress main file to be able to access WP functions include(&#39;blog/wp-load.php&#39;); // Define the scope of the search for posts, pages (published, drafts ... ) $args = array( &#39;post_type&#39; =&amp;gt; &#39;post&#39;, &#39;post_status&#39; =&amp;gt; &#39;publish&#39;, &#39;posts_per_page&#39; =&amp;gt; -1); $posts_array = get_posts( $args ); echo &quot; We have found &quot;. count($posts_array) . &quot; posts&quot;;// Simple counter to check the number of &quot;infected&quot; posts $count = 0; // Start the WP loop in order to check posts foreach( $posts_array as $post ) {  setup_postdata($post);  ob_start();ob_end_clean();  // Here we define the search parameters .. you can specify HTML tags here  $output = preg_match_all(&#39;/&amp;lt;blockquote.*&amp;gt;/&#39;, $post-&amp;gt;post_content, $matches);  // You can specify a set of characters or Strings here  $output = strstr($post-&amp;gt;post_content, &#39;zemanta-pixie&#39;);  $post_categories = wp_get_post_categories( $post-&amp;gt;ID );  // Another type of filtering .. checking post categories and number of posts  $output =  in_category( &#39;miscellaneous&#39; ) &amp;amp;&amp;amp; count($post_categories) == 1;  if ($output) {print_r($matches);echo &quot;We have found code in post: &quot; . $post-&amp;gt;ID . &quot; name: &quot;. $post-&amp;gt;post_name .&quot; &amp;lt;a href=&#39;http://ahmadassaf.com/blog/wp-admin/post.php?post=&quot; . $post-&amp;gt;ID.&quot;&amp;amp;action=edit&#39;&amp;gt;Link&amp;lt;/a&amp;gt;&amp;lt;/br&amp;gt;&quot;;$count++;  } } echo &quot;Total of: &quot; . $count . &quot;&amp;lt;/br&amp;gt;&quot;;?&amp;gt;",
      "url"      : "/how-to-search-all-your-wordpress-posts-for-certain-characters-strings-or-codes/",
      "date"     : "2013-10-22 00:00:00 +0100"
    },
  
    {
      "title"    : "How to get all the tags for a WordPress Category ?",
      "category" : "web development",
      "content"  : "For various reasons, one might want to get all the categories that are assigned for the posts in a certain category. This snippet below will allow to do just that. What i did is simply putting that in the functions and then you get to call that function anywhere in the code. The parameter that you need to pass is the category ID. Please note that the table names that you have may have different prefix, the default is the wp_ and that we pull in the tags from published posts only. You can change that by changing the post status parameter. This code retrieves the tags based on the number of their occurrences in the posts in DESCENDING order. You can change that as well by changing the order to ASC&amp;lt;?php function get_category_tags($args) {  global $wpdb;  $tags = $wpdb-&amp;gt;get_results  (&quot;SELECT DISTINCT terms2.term_id as tag_id, terms2.name as tag_name, t2.count as posts_with_tagFROMwp_posts as p1LEFT JOIN wp_term_relationships as r1 ON p1.ID = r1.object_IDLEFT JOIN wp_term_taxonomy as t1 ON r1.term_taxonomy_id = t1.term_taxonomy_idLEFT JOIN wp_terms as terms1 ON t1.term_id = terms1.term_id,wp_posts as p2LEFT JOIN wp_term_relationships as r2 ON p2.ID = r2.object_IDLEFT JOIN wp_term_taxonomy as t2 ON r2.term_taxonomy_id = t2.term_taxonomy_idLEFT JOIN wp_terms as terms2 ON t2.term_id = terms2.term_idWHEREt1.taxonomy = &#39;category&#39; AND p1.post_status = &#39;publish&#39; AND terms1.term_id IN (&quot;.$args.&quot;) ANDt2.taxonomy = &#39;post_tag&#39; AND p2.post_status = &#39;publish&#39;AND p1.ID = p2.IDORDER by posts_with_tag DESC&quot;);  return $tags; }?&amp;gt;",
      "url"      : "/how-to-get-all-the-tags-for-a-wordpress-category/",
      "date"     : "2013-10-11 00:00:00 +0100"
    },
  
    {
      "title"    : "Get rid of unused attachments",
      "category" : "web development",
      "content"  : "A very annoying thing in WordPress is that you deleting a post doesn’t cascade to the content (attachments) of that post. After a while, you can end up with lots of unused attachments and media files in your library that will only take up hard drive space ! The function below will allow deletion of post attachments when the post is deleted.&amp;lt;?php function delete_posts_before_delete_post($id){  $subposts = get_children(array(&#39;post_parent&#39; =&amp;gt; $id,&#39;post_type&#39;=&amp;gt; &#39;any&#39;,&#39;numberposts&#39; =&amp;gt; -1,&#39;post_status&#39; =&amp;gt; &#39;any&#39;));  if (is_array($subposts) &amp;amp;&amp;amp; count($subposts) &amp;gt; 0){$uploadpath = wp_upload_dir();foreach($subposts as $subpost){ $_wp_attached_file = get_post_meta($subpost-&amp;gt;ID, &#39;_wp_attached_file&#39;, true); $original = basename($_wp_attached_file); $pos = strpos(strrev($original), &#39;.&#39;); if (strpos($original, &#39;.&#39;) !== false){  $ext = explode(&#39;.&#39;, strrev($original));  $ext = strrev($ext[0]); } else {  $ext = explode(&#39;-&#39;, strrev($original));  $ext = strrev($ext[0]); } $pattern = $uploadpath[&#39;basedir&#39;].&#39;/&#39;.dirname($_wp_attached_file).&#39;/&#39;.basename($original, &#39;.&#39;.$ext).&#39;-[0-9]*x[0-9]*.&#39;.$ext; $original= $uploadpath[&#39;basedir&#39;].&#39;/&#39;.dirname($_wp_attached_file).&#39;/&#39;.basename($original, &#39;.&#39;.$ext).&#39;.&#39;.$ext; if (getimagesize($original)){  $thumbs = glob($pattern);  if (is_array($thumbs) &amp;amp;&amp;amp; count($thumbs) &amp;gt; 0){foreach($thumbs as $thumb) unlink($thumb);  } } wp_delete_attachment( $subpost-&amp;gt;ID, true );}  } }?&amp;gt;Now, we just need to add some hooks in order to run this function automatically when a delete action is triggered.### Up-To WordPress 3.1add_action(&#39;delete_post&#39;, &#39;delete_posts_before_delete_post&#39;);### From WordPress 3.2add_action(&#39;before_delete_post&#39;, &#39;delete_posts_before_delete_post&#39;);",
      "url"      : "/delete-wordpress-attachments/",
      "date"     : "2013-07-04 00:00:00 +0100"
    }
  
]